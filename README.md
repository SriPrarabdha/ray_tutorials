
# A practical workshop on scaling ML workflows with the Ray ecosystem


## üöÄ Overview

Modern AI systems are no longer bottlenecked by *models* ‚Äî they are bottlenecked by **infrastructure**. Training models, processing terabytes of data, deploying LLMs, and orchestrating GPU clusters all require tooling that simplifies distributed systems.

**Ray** is that tooling.

This repository contains everything used in my **60-minute workshop on AI Infrastructure with Ray**.
Each folder contains:

* A **baseline implementation** using traditional Python / PyTorch / multiprocessing
* A **Ray-powered implementation** showing how the same workflow becomes scalable, cleaner, and fault-tolerant

If you are new to Ray, this repo will help you understand not just APIs ‚Äî
but *how Ray changes the way ML engineers build systems*.


# üìö What You‚Äôll Learn

Inside this repo you will learn how to:

### üîπ **Scale Python code to clusters without rewriting it**

Ray Tasks & Actors turn normal Python functions into distributed workloads effortlessly.

### üîπ **Process huge datasets with Ray Data**

Load, stream, preprocess, and batch terabytes of text or images using a completely Pythonic interface.

### üîπ **Train models across multiple GPUs & nodes**

Use Ray Train to scale PyTorch/HuggingFace/PEFT models with fault tolerance and automatic checkpointing.

### üîπ **Serve ML models (including LLMs) in production**

Ray Serve lets you deploy, autoscale, route, batch, and version models ‚Äî including vLLM deployments.

### üîπ **Run high-throughput LLM inference**

Use vLLM with Ray Serve for blazing-fast, production-grade LLM inference.

---

# üóÇ Repository Structure

```
ray_tutorials/
‚îú‚îÄ‚îÄ ray_core/
‚îÇ   ‚îú‚îÄ‚îÄ baseline              # Standard Python multiprocessing / threading examples
‚îÇ   ‚îú‚îÄ‚îÄ ray_tasks             # Same examples rewritten using Ray Tasks & Actors
‚îÇ   ‚îî‚îÄ‚îÄ ray_actors            # How to run on real Ray clusters (VMs, LAN, K8s)
‚îÇ
‚îú‚îÄ‚îÄ ray_data/
‚îÇ   ‚îú‚îÄ‚îÄ baseline              # Pandas, plain Python data pipelines
‚îÇ   ‚îî‚îÄ‚îÄ ray_version           # Ray Data: distributed loading, batching, streaming
‚îÇ
‚îú‚îÄ‚îÄ ray_train/
‚îÇ   ‚îú‚îÄ‚îÄ baseline              # Single-GPU PyTorch training (DDP optional)
‚îÇ   ‚îî‚îÄ‚îÄ ray_version           # Ray Train distributed training, FT, checkpoints
‚îÇ
‚îú‚îÄ‚îÄ ray_serve/
‚îÇ   ‚îú‚îÄ‚îÄ baseline              # Simple Flask/FastAPI serving patterns
‚îÇ   ‚îî‚îÄ‚îÄ ray_version           # Ray Serve deployments, autoscaling, routing, batching
‚îÇ
‚îú‚îÄ‚îÄ ray_tune/
‚îÇ   ‚îî‚îÄ‚îÄ examples              # will be added soon
‚îÇ
‚îî‚îÄ‚îÄ vllm_examples/
    
```

Every module includes:

‚úî **Baseline Python code**
‚úî **Ray implementation**
‚úî **Explanations + comments**
‚úî **Cluster-ready examples**

---

# üß© Who Is This For?

### üéì **Students & Researchers**

* Learn how to scale experiments without rewriting everything
* Build reproducible ML pipelines
* Run multi-GPU training in your lab or on the cloud

### üõ† **ML Engineers**

* Build data pipelines, training pipelines, and serving pipelines
* Turn your laptop code into distributed code
* Deploy LLMs with autoscaling and batching

### üè¢ **Tech Teams / Startups**

* Build production ML infra without managing complex distributed systems
* Replace 5‚Äì6 tools with a unified Ray-based workflow
* Save engineering time and avoid infrastructure glue code

---

# üèó Philosophy of This Workshop

This workshop is built around a simple idea:

> **‚ÄúDistributed ML should not require learning distributed systems.‚Äù**

Ray lets you scale your code using:

* your **existing Python functions**
* your **existing PyTorch models**
* your **existing HuggingFace workflows**
* your **existing serving patterns**

No MPI.
No Kubernetes YAML.
No Spark jobs.
No complicated Docker setups (unless you want them).

You get the power of a large-scale distributed system
**with the simplicity of standard Python.**

# ü§ù Contributions

Feel free to open issues or PRs if you have improvements, bug fixes, or new examples.

---

# ‚≠ê Acknowledgments

This workshop and repo were created as part of a technical session at **UbuCon India 2025**, showing how open-source AI infrastructure enables even small teams to operate like large ML organizations.

---
